{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models.video as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def task1(file_path,model_name):\n",
    "    \n",
    "    #visualise the video\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    if(cap.isOpened() == False):\n",
    "        print(\"error... cant open video\")\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            cv2.imshow('Frame',frame)\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else: break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    def getFrames(file_path):\n",
    "        '''\n",
    "        Reads the video frame by frame and returns a list of frames\n",
    "        '''\n",
    "        video = cv2.VideoCapture(file_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "        video.release()\n",
    "        return frames\n",
    "\n",
    "    def processFrames(frames,t):\n",
    "        '''\n",
    "        Processes and transforms the frames based on the transformation (t) provided\n",
    "        '''\n",
    "        processed = []\n",
    "        for frame in frames:\n",
    "            frame = t(frame)\n",
    "            processed.append(frame)\n",
    "        return torch.stack(processed)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112,112)),\n",
    "        transforms.ToTensor()  \n",
    "    ])\n",
    "\n",
    "    frames = getFrames(file_path)\n",
    "    pro = processFrames(frames,transform)\n",
    "    pro = pro.unsqueeze(0) # Add batch dimension , shape becomes [N,D,C,H,W] , N-batch, D-Depth, C-channels, Height(H) and Width(W) of frame\n",
    "    pro1 = torch.movedim(pro,1,2) # convert to shape [N,C,D,H,W] which is the required input shape in r3d \n",
    "    \n",
    "    def hook(module,input,output):\n",
    "        global features\n",
    "        features = output\n",
    "\n",
    "    def initialize_model(model_name):\n",
    "        '''\n",
    "        Add hooks to the layer based on the model name provided\n",
    "        '''\n",
    "        global features\n",
    "        model = models.r3d_18(pretrained=True)\n",
    "        model.eval()\n",
    "        if model_name == \"layer3\":\n",
    "            h1 = model.layer3.register_forward_hook(hook)\n",
    "        elif model_name == \"layer4\":\n",
    "            h1 = model.layer4.register_forward_hook(hook)\n",
    "        elif model_name == \"avgpool\":\n",
    "            h1 = model.avgpool.register_forward_hook(hook)\n",
    "        # pro1 = pro[0][None,:,:,:]\n",
    "        out = model(pro1)\n",
    "        h1.remove()\n",
    "\n",
    "    initialize_model(model_name)\n",
    "\n",
    "    if(features.shape[1] == 256): \n",
    "        # if model is layer 3, average spatial dimensions and then flatten the tensor followed by a linear transformation to get 512 dimensional tensor\n",
    "        avg_features = torch.mean(features,dim=(3,4))\n",
    "        squeezed = torch.squeeze(avg_features) #remove batch dimension\n",
    "        in_tensor = torch.flatten(squeezed) #collapse into a single dimension \n",
    "        myLayer = nn.Linear(in_features=256*features.shape[2],out_features=512) # define a linear layer \n",
    "        final_tensor = myLayer(torch.squeeze(in_tensor)) #remove batch dimension with squeeze and then apply linear transformation\n",
    "    elif(features.shape[1] == 512 and features.shape[2] != 1): # case of layer 4\n",
    "        # average the tensor on dimension 2,3,4 to get 512 dimensional tensor\n",
    "        final_tensor = torch.squeeze(torch.mean(features,dim=(2,3,4)))\n",
    "    else: #case of avgpool\n",
    "        #this layer will already give output as 512 dimensional tensor\n",
    "        final_tensor = torch.squeeze(features)\n",
    "    \n",
    "    return final_tensor\n",
    "\n",
    "\n",
    "# path = 'C:\\\\Users\\\\aryan\\\\Desktop\\\\ASU\\\\CSE 515\\\\Target_videos\\\\cartwheel\\\\cartwheel\\\\Acrobacias_de_un_fenomeno_cartwheel_f_cm_np1_ba_bad_8.avi'\n",
    "# model = \"avgpool\"\n",
    "\n",
    "# task1(path,model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Task2(file_path,model_name):\n",
    "    \n",
    "    def load_all_cluster_representatives_hog():\n",
    "        cluster_representatives = {}\n",
    "        sigma2_values = [4, 8, 16, 32, 64, 128]\n",
    "        tau2_values = [2, 4]\n",
    "        pair_index = 0\n",
    "        for sigma2 in sigma2_values:\n",
    "            for tau2 in tau2_values:\n",
    "                centroids_file = f'../HoG/pair_{sigma2}_{tau2}_HoG.csv'\n",
    "                centroids = np.loadtxt(centroids_file, delimiter=',')\n",
    "                cluster_representatives[(sigma2, tau2)] = centroids\n",
    "                pair_index += 1\n",
    "        return cluster_representatives\n",
    "    \n",
    "    def load_all_cluster_representatives_hof():\n",
    "        cluster_representatives = {}\n",
    "        sigma2_values = [4, 8, 16, 32, 64, 128]\n",
    "        tau2_values = [2, 4]\n",
    "        pair_index = 0\n",
    "        for sigma2 in sigma2_values:\n",
    "            for tau2 in tau2_values:\n",
    "                centroids_file = f'../HoF/pair_{sigma2}_{tau2}_HoF.csv'\n",
    "                centroids = np.loadtxt(centroids_file, delimiter=',')\n",
    "                cluster_representatives[(sigma2, tau2)] = centroids\n",
    "                pair_index += 1\n",
    "        return cluster_representatives\n",
    "    \n",
    "    def extract_features(file_name):\n",
    "        data_array = []\n",
    "        with open(file_name, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            for row in csvreader:\n",
    "                data_array.append([float(x) for x in row])\n",
    "        data_array = np.array(data_array)       \n",
    "        return data_array\n",
    "\n",
    "    def get_sigma2_tau2_pair(row):\n",
    "        sigma2 = row[4]\n",
    "        tau2 = row[5]   \n",
    "        return (sigma2, tau2)\n",
    "\n",
    "    def assign_row_to_cluster(features_row, cluster_representatives, sigma2_tau2_pair):\n",
    "        centroids = cluster_representatives[sigma2_tau2_pair] \n",
    "        distances = cdist([features_row], centroids, 'euclidean') \n",
    "        closest_cluster = np.argmin(distances) \n",
    "        return closest_cluster\n",
    "\n",
    "\n",
    "    def create_and_concatenate_histograms(hist_data):\n",
    "        expected_pairs = [\n",
    "            (4, 2), (4, 4), (8, 2), (8, 4), (16, 2), (16, 4), \n",
    "            (32, 2), (32, 4), (64, 2), (64, 4), (128, 2), (128, 4)\n",
    "        ]\n",
    "        grouped_indices = {pair: [] for pair in expected_pairs}\n",
    "        for (sigma2_tau2, index) in hist_data:\n",
    "            grouped_indices[sigma2_tau2].append(index)\n",
    "        histograms = []\n",
    "        for sigma2_tau2, indices in grouped_indices.items():\n",
    "            histogram, _ = np.histogram(indices, bins=np.arange(41), density=False)\n",
    "            histograms.append(histogram)\n",
    "        if len(histograms) != 12:\n",
    "            raise ValueError(f\"Expected 12 histograms, but found {len(histograms)}.\")\n",
    "        concatenated_vector = np.hstack(histograms)\n",
    "        return concatenated_vector\n",
    "\n",
    "    stips_folder = \"../Assets/hmdb51_org_stips_filtered\"\n",
    "    action_subfolder = os.path.basename(os.path.dirname(file_path))  # Get the subfolder (action) name\n",
    "    video_name = os.path.basename(file_path)  # Get the video filename (e.g., videoname.avi)\n",
    "    # Create the corresponding STIP file name by appending '.csv' to the video filename\n",
    "    stip_file_name = f\"{video_name}.csv\"\n",
    "    # Construct the full path to the STIP file\n",
    "    video_stip_path = os.path.join(stips_folder, action_subfolder, stip_file_name)\n",
    "    \n",
    "    def Task2b():\n",
    "        cluster_representatives = load_all_cluster_representatives_hog()\n",
    "        stip_features = extract_features(video_stip_path)\n",
    "        hist_data = []\n",
    "        for row in stip_features:\n",
    "            ind1 = assign_row_to_cluster(row[7:79], cluster_representatives, get_sigma2_tau2_pair(row))\n",
    "            hist_data.append([get_sigma2_tau2_pair(row), ind1])\n",
    "        # create_histogram_for_pairs(hist_data)\n",
    "        bog_hog_480 = create_and_concatenate_histograms(hist_data)\n",
    "        # print(\"Concatenated 480-dimensional vector:\", bog_hog_480)\n",
    "        # print(\"Shape of the concatenated vector:\", bog_hog_480.shape)\n",
    "        return bog_hog_480\n",
    "    \n",
    "    def Task2c():\n",
    "        cluster_representatives = load_all_cluster_representatives_hof()\n",
    "        stip_features = extract_features(video_stip_path)\n",
    "        hist_data = []\n",
    "        for row in stip_features:\n",
    "            ind1 = assign_row_to_cluster(row[79:], cluster_representatives, get_sigma2_tau2_pair(row))\n",
    "            hist_data.append([get_sigma2_tau2_pair(row), ind1])\n",
    "        # Concatenate the 12 histograms into a 480-dimensional vector\n",
    "        bog_hof_480 = create_and_concatenate_histograms(hist_data)\n",
    "        # print(\"Concatenated 480-dimensional vector:\", bog_hof_480)\n",
    "        # print(\"Shape of the concatenated vector:\", bog_hof_480.shape)\n",
    "        return bog_hof_480\n",
    "\n",
    "    if model_name == \"hog\":\n",
    "        final = Task2b()\n",
    "        return final\n",
    "    elif model_name ==\"hof\":\n",
    "        final = Task2c()\n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def task3(path, r, n, frames_folder='Frames', feature_folder='Features'):\n",
    "\n",
    "    os.makedirs(frames_folder, exist_ok=True) # Folder for saving frames\n",
    "    os.makedirs(feature_folder, exist_ok=True)  # Folder for saving features\n",
    "\n",
    "    base_name = os.path.basename(path).split('.')[0] # This is to get the video name and not path\n",
    "\n",
    "    cam = cv2.VideoCapture(path)\n",
    "    \n",
    "    # Here we are counting total frames in the video\n",
    "    total_frames=int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    frameno = 0\n",
    "    while(True):\n",
    "\n",
    "        ret,frame = cam.read()\n",
    "        if ret:\n",
    "            frameno=frameno+1  # Counter for frames\n",
    "            if(frameno==1):  # If and else condition for reaching first,middle and last frame and saving them\n",
    "                frame_name = os.path.join(frames_folder, f'{base_name}_frame_1.jpg')  # Save with base name\n",
    "                print(\"First frame: \"+frame_name)\n",
    "                cv2.imwrite(frame_name,frame)\n",
    "            elif (frameno==int(total_frames/2)):\n",
    "                frame_name = os.path.join(frames_folder, f'{base_name}_frame_2.jpg')  # Save with base name\n",
    "                print(\"Middle frame: \"+frame_name)\n",
    "                cv2.imwrite(frame_name,frame)\n",
    "            elif (frameno==int(total_frames)-1):\n",
    "                frame_name = os.path.join(frames_folder, f'{base_name}_frame_3.jpg')  # Save with base name\n",
    "                print(\"Last frame: \"+frame_name)\n",
    "                cv2.imwrite(frame_name,frame)\n",
    "                # Will skip rest of the frames\n",
    "        else:\n",
    "            break # If no more frames are found in video we exit\n",
    "    \n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "#     r = 4  # here we can take value of r and n as inputs incase if user wants to define them\n",
    "#     n = 12\n",
    "    histos = []\n",
    "    video_feature_vector = [] # This is a list for feature vectors of concatenated histograms\n",
    "    \n",
    "    for frame_index in range(1, 4):  # first loop to go through first,middle and last frame\n",
    "        image_name = os.path.join(frames_folder, f'{base_name}_frame_{frame_index}.jpg')\n",
    "        img = cv2.imread(image_name)\n",
    "        if img is None:\n",
    "            print(f\"Error reading {image_name}. File may not exist or is corrupted.\")\n",
    "            continue  # Skip to the next frame\n",
    "        \n",
    "        im_h, im_w, channels = img.shape  # Reading the image and extracting its height and width\n",
    "        \n",
    "        figure, axis = plt.subplots(r, r)  # This is for creating figure of attached cell histograms\n",
    "        plt.subplots_adjust(top=1,bottom=0.5,right=1,left=0.5)\n",
    "        \n",
    "        # Feature vector for the current frame\n",
    "        frame_feature_vector = []\n",
    "        \n",
    "        for i in range(1,r+1): # This loop is for going from top to bottom of image\n",
    "            for j in range(1,r+1):  # This loop is for going from left to right of image\n",
    "                tile = img[(im_h//r)*(i-1):(im_h//r)*i,(im_w//r)*(j-1):(im_w//r)*j]# This is cutting\n",
    "                # out cells based on dimensions of the image\n",
    "                rgb_tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB) # Converting cells/tiles from bgr to rgb\n",
    "                \n",
    "                # Now we calculate histograms for red,green and blue channels of the cell\n",
    "                hist_r = np.histogram(rgb_tile[:,:,0].ravel(),bins=n)[0]\n",
    "                hist_g = np.histogram(rgb_tile[:,:,1].ravel(),bins=n)[0]\n",
    "                hist_b = np.histogram(rgb_tile[:,:,2].ravel(),bins=n)[0]\n",
    "                # Concatenate histograms for all three channels into a single vector for this tile\n",
    "                tile_histogram = np.concatenate([hist_r, hist_g, hist_b])\n",
    "                # Append this tile's histogram to the frame feature vector\n",
    "                frame_feature_vector.extend(tile_histogram)\n",
    "                \n",
    "                # Now we plot the histograms for n bins for tile and for red,green blue channel\n",
    "                axis[i-1,j-1].hist(rgb_tile[:,:,0].ravel(),bins=n,edgecolor='black',color='red',alpha=0.5)\n",
    "                axis[i-1,j-1].hist(rgb_tile[:,:,1].ravel(),bins=n,edgecolor='black',color='green',alpha=0.5)\n",
    "                axis[i-1,j-1].hist(rgb_tile[:,:,2].ravel(),bins=n,edgecolor='black',color='blue',alpha=0.5)\n",
    "                axis[i-1,j-1].tick_params(axis='both',labelsize=4)\n",
    "        \n",
    "        # Append this frame's feature vector to the video feature vector\n",
    "        video_feature_vector.extend(frame_feature_vector)\n",
    "        \n",
    "        # following is to save the histogram based on details like timestamp and video it belongs to\n",
    "        timestamp = int(time.time())  # here I used timestamp because if the name of histograms are same than it will\n",
    "        # overwrite the saved histogram so with timestamp it gives unique name to each saved file\n",
    "        hist_name = f'{base_name}_histogram_frame_{frame_index}_{timestamp}.png'\n",
    "        hist_path = os.path.join(\"Outputs/Task3/Histograms_Framewise\", hist_name)\n",
    "        plt.savefig(hist_path)\n",
    "        plt.close(figure)  # Close the figure after saving\n",
    "        \n",
    "        histos.append(hist_path)\n",
    "        \n",
    "    video_feature_vector = np.array(video_feature_vector) # Here we convert the video feature vector to np array\n",
    "    # following is to save the feature vector\n",
    "    feature_file = os.path.join(feature_folder,f'{base_name}_features.npy')\n",
    "    np.save(feature_file,video_feature_vector)\n",
    "    \n",
    "    print(f\"Saved feature vector for video '{base_name}' to {feature_file}\")\n",
    "    \n",
    "    return video_feature_vector, histos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Assets/hmdb51_org/target_videos\\cartwheel\\(Rad)Schlag_die_Bank!_cartwheel_f_cm_np1_le_med_0.avi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the main folder containing action subfolders\n",
    "main_folder = '../Assets/hmdb51_org/target_videos'\n",
    "\n",
    "# List to store the paths of all video files\n",
    "video_paths = []\n",
    "\n",
    "# Walk through the folder structure\n",
    "for root, dirs, files in os.walk(main_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        video_paths.append(full_path)\n",
    "\n",
    "print(video_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2(video_paths[0],\"hof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each target video, perform all 3 tasks and store their respective outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../Assets/hmdb51_org/target_videos\\cartwheel\\(Rad)Schlag_die_Bank!_cartwheel_f_cm_np1_le_med_0.avi...\n",
      "Task 1 (layer3)....\n",
      "Saved Task 1 (layer3) features to Outputs/Task1\\layer3\\(Rad)Schlag_die_Bank!_cartwheel_f_cm_np1_le_med_0_features.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Task1: Process the video for all 3 models\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m task1_models:\n\u001b[1;32m---> 38\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m task1(video_path, model)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask 1 (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m, in \u001b[0;36mtask1\u001b[1;34m(file_path, model_name)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m'\u001b[39m,frame)\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m25\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "output_dir1 = \"Outputs/Task1\"\n",
    "os.makedirs(output_dir1, exist_ok=True)  #Create directory if it doesn't exist\n",
    "task1_models = [\"layer3\",\"layer4\",\"avgpool\"] # There are 3 visual models in task 1\n",
    "for model in task1_models:\n",
    "    os.makedirs(os.path.join(output_dir1, model), exist_ok=True)\n",
    "\n",
    "output_dir2 = \"Outputs/Task2\"\n",
    "os.makedirs(output_dir2, exist_ok=True)  \n",
    "task2_models = [\"hog\",\"hof\"]\n",
    "for model in task2_models:\n",
    "    os.makedirs(os.path.join(output_dir2, model), exist_ok=True)\n",
    "    \n",
    "# Saving Features for Task 2\n",
    "def save_vector_to_file(vector, output_path):\n",
    "\n",
    "    reshaped_vector = vector.reshape(1, -1)\n",
    "    \n",
    "    reshaped_vector_int = reshaped_vector.astype(int)\n",
    "    \n",
    "    np.savetxt(output_path, reshaped_vector_int, delimiter=',', fmt='%d')\n",
    "\n",
    "\n",
    "output_dir3 = \"Outputs/Task3\"\n",
    "os.makedirs(output_dir3, exist_ok=True)\n",
    "task3_folders = [\"Histograms_Framewise\",\"Feature_Vector\"]\n",
    "for model in task3_folders:\n",
    "    os.makedirs(os.path.join(output_dir3, model), exist_ok=True)\n",
    "\n",
    "for video_path in video_paths[:2]:\n",
    "    print(f\"Processing {video_path}...\")\n",
    "\n",
    "    # Task1: Process the video for all 3 models\n",
    "    for model in task1_models:\n",
    "        output_tensor = task1(video_path, model)\n",
    "        if output_tensor is not None:\n",
    "            print(f\"Task 1 ({model})....\")\n",
    "            # Create a filename for the output tensor based on the input video name\n",
    "            video_name = os.path.basename(video_path).split('.')[0]\n",
    "            # Save the output tensor in the corresponding model folder\n",
    "            output_path = os.path.join(output_dir1, model, f\"{video_name}_features.pt\")\n",
    "            torch.save(output_tensor, output_path)\n",
    "            print(f\"Saved Task 1 ({model}) features to {output_path}\")\n",
    "\n",
    "    # Task2: Process the video and save the output\n",
    "    for model in task2_models:\n",
    "        output_2 = Task2(video_path,model)\n",
    "        if output_2 is not None:\n",
    "            print(\"Task 2....\")\n",
    "            video_name = os.path.basename(video_path).split('.')[0]\n",
    "            output_path = os.path.join(output_dir2, model, f\"{video_name}.csv\")\n",
    "            save_vector_to_file(output_2, output_path)\n",
    "            # torch.save(output_2, output_path)\n",
    "            print(f\"Saved Task 2 features to {output_path}\")\n",
    "\n",
    "    # Task3: Process the video and save the output\n",
    "    feature_folder = os.path.join(output_dir3, \"Feature_Vector\")\n",
    "    output_3, histograms = task3(video_path, 4, 12, frames_folder='frames_fold', feature_folder=feature_folder)\n",
    "    if output_3 is not None:\n",
    "        print(f\"Video feature vector shape: {output_3.shape}\")\n",
    "        print(f\"Histograms saved: {histograms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
